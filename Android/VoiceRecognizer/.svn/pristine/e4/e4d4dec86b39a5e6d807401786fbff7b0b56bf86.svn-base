package tum.laser.voicerecognizer;

import java.io.BufferedOutputStream;
import java.io.DataOutputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.FileWriter;
import java.io.IOException;
import java.io.OutputStream;
import java.io.PrintWriter;
import java.math.BigDecimal;
import java.math.RoundingMode;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.LinkedList;

import tum.laser.javagmm.FFT;
import tum.laser.javagmm.FrequencyProperties;
import tum.laser.javagmm.GaussianMixture;
import tum.laser.javagmm.MFCC;
import tum.laser.javagmm.PointList;
import tum.laser.javagmm.Window;

import android.media.AudioFormat;
import android.media.AudioRecord;
import android.media.MediaRecorder;
import android.os.Bundle;
import android.os.Environment;
import android.app.Activity;
import android.util.Log;
import android.view.Menu;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.ImageView;
import android.widget.Switch;
import android.widget.Toast;

/**
 * VoiceRecognizer
 * A speech processor for the Android plattform.
 * 
 * @author Florian Schulze, schulze@in.tum.de, 2013
 * 
 *
 * contains code taken from:
 * 
 * Funf: Open Sensing Framework
 * Copyright (C) 2010-2011 Nadav Aharony, Wei Pan, Alex Pentland.
 * Acknowledgments: Alan Gardner
 * Contact: nadav@media.mit.edu
 */
public class MainActivity extends Activity {

	private static String APP_NAME = "VoiceRecognizer";

	private static int RECORDER_SOURCE = MediaRecorder.AudioSource.VOICE_RECOGNITION;
	private static int RECORDER_CHANNELS = AudioFormat.CHANNEL_IN_MONO;
	private static int RECORDER_AUDIO_ENCODING = AudioFormat.ENCODING_PCM_16BIT;
	private static int RECORDER_SAMPLERATE = 8000;

	private static int WINDOWS_TO_RECORD = 4;

	private static int FFT_SIZE = 512;
	//8000samples/s divided by 256samples/frame -> 32ms/frame (31.25ms)
	private static int FRAME_SIZE_IN_SAMPLES = 256;

	//32ms/frame times 64frames/window = 2s/window
	private static int WINDOW_SIZE_IN_FRAMES = 64;

	private static int MFCCS_VALUE = 20;
	private static int NUMBER_OF_FINAL_FEATURES = MFCCS_VALUE - 1; //discard energy
	private static int MEL_BANDS = 20; //use FB-20
	private static double[] FREQ_BANDEDGES = {50,250,500,1000,2000};

	private int bufferSize = 0;

	private static int[] freqBandIdx = null;
	public double[] featureBuffer = null;

	private Thread recordingThread = null;
	private FFT featureFFT = null;
	private MFCC featureMFCC = null;
	private Window featureWin = null;
	private AudioRecord audioRecorder = null;

	private boolean isRecording = false;
	
	private static int DROP_FIRST_X_WINDOWS = 1;

	//TODO do this properly
	GaussianMixture gmmFlo = null;
	GaussianMixture gmmOther = null;
	ImageView recognition = null;
	ImageView admission = null;


	/**
	 * Start recording audio in a separate thread.
	 * If the switch in our main view is checked, the audio will be saved in
	 * a file {@link #storeAudioStream()}. Otherwise we process the stream
	 * immediately {@link #processAudioStream()}.
	 */
	protected void recordAudioFromMic() {
		synchronized (this) {
			if (isRecording)
				return;
			else
				isRecording = true;
		}
		bufferSize = AudioRecord.getMinBufferSize(
				RECORDER_SAMPLERATE,
				RECORDER_CHANNELS,
				RECORDER_AUDIO_ENCODING);

		bufferSize = Math.max(bufferSize, RECORDER_SAMPLERATE*2);
		audioRecorder = new AudioRecord(
				RECORDER_SOURCE,
				RECORDER_SAMPLERATE,
				RECORDER_CHANNELS,
				RECORDER_AUDIO_ENCODING,
				bufferSize);
		audioRecorder.startRecording();
		recordingThread = new Thread(new Runnable()
		{
			@Override
			public void run()
			{
				Switch s = (Switch) findViewById(R.id.switchAudio);
				try {
					Thread.sleep(3000);
				} catch (InterruptedException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				if (s.isChecked())
					storeAudioStream();
				else
					processAudioStream();
			}
		}, APP_NAME + "_Thread");
		recordingThread.start();
	}


	/**
	 * Stop recording. Kills off any ongoing recording.
	 */
	protected void stopRecording() {
		synchronized (this) {
			if (!isRecording)
				return;
			else
				isRecording = false;
		}

		Toast.makeText(getApplicationContext(), "Stopped recording. Resetting...", Toast.LENGTH_SHORT).show();

		audioRecorder.stop();
		audioRecorder.release();
		audioRecorder = null;
		recordingThread = null;
	}

	@Override
	protected void onStop() {
		super.onStop();
		//stopRecording();
	}
	
	@Override
	protected void onCreate(Bundle savedInstanceState) {
		super.onCreate(savedInstanceState);
		setContentView(R.layout.activity_main);

		final Button buttonStart = (Button) findViewById(R.id.buttonStartRec);
		buttonStart.setOnClickListener(new View.OnClickListener() {
			public void onClick(View v) {
				recordAudioFromMic();
			}
		});

		final Button buttonStop = (Button) findViewById(R.id.buttonStopRec);
		buttonStop.setOnClickListener(new View.OnClickListener() {
			public void onClick(View v) {
				stopRecording();
			}
		});
		
		if (gmmFlo == null) {
			gmmFlo = GaussianMixture.readGMM(Environment.getExternalStorageDirectory().getAbsolutePath() + "/" +"gmmFlo");
			gmmOther = GaussianMixture.readGMM(Environment.getExternalStorageDirectory().getAbsolutePath() + "/" +"gmmOther");
	
			Toast.makeText(getApplicationContext(), "GMMs loaded", Toast.LENGTH_SHORT).show();
		}
		recognition = (ImageView) findViewById(R.id.recognitionView);
		admission = (ImageView) findViewById(R.id.admissionView);
	}

	@Override
	public boolean onCreateOptionsMenu(Menu menu) {
		// Inflate the menu; this adds items to the action bar if it is present.
		getMenuInflater().inflate(R.menu.main, menu);
		return true;
	}


	
	
	//TODO: drop frames without speech -> speakersense
	//TODO: cepstral mean normalization -> overview paper
	//TODO: augment features with derivatives -> overview paper
	//TODO: buffer in-between frames vs drop; drop whole window vs frames

	/**
	 * Reads from {@link #audioRecorder} and processes the stream.
	 * For that, it cuts the stream into chunks (frames) which are
	 * processed individually in
	 * {@link #processAudioFrame(int, short[], double[], double[])}
	 * to obtain feature vectors.
	 * Features of multiple frames are then bundled to feature windows
	 * which represent speech utterances.
	 * Before we add a frame to a window we run admission checks.
	 * Frames that are not admitted simply get dropped. Therefore,
	 * a window contains sequential but not necessarily consecutive
	 * frames.
	 * 
	 * v1.0
	 */
	private void processAudioStream()
	{
		short dataFrame16bit[] = new short[FRAME_SIZE_IN_SAMPLES];

		//initialize general processing data
		featureWin = new Window(FRAME_SIZE_IN_SAMPLES); //smoothing window, nothing to do with frame window
		featureFFT = new FFT(FFT_SIZE);
		featureMFCC = new MFCC(FFT_SIZE, MFCCS_VALUE, MEL_BANDS, RECORDER_SAMPLERATE);

		freqBandIdx = new int[FREQ_BANDEDGES.length];
		for (int i = 0; i < FREQ_BANDEDGES.length; i ++)
		{
			freqBandIdx[i] = Math.round((float)FREQ_BANDEDGES[i]*((float)FFT_SIZE/(float)RECORDER_SAMPLERATE));
		}

		//lists of windows
		LinkedList<ArrayList<double[]>> featureCepstrums = new LinkedList<ArrayList<double[]>>();
		LinkedList<ArrayList<double[]>> psdsAcrossFrequencyBands = new LinkedList<ArrayList<double[]>>();

		//windows: list of frames
		ArrayList<double[]> cepstrumWindow = new ArrayList<double[]>(WINDOW_SIZE_IN_FRAMES);
		ArrayList<double[]> psdWindow = new ArrayList<double[]>(WINDOW_SIZE_IN_FRAMES);

		int readAudioSamples = 0;
		int currentIteration = 0;

		String editText = ((EditText) findViewById(R.id.editTextSamples)).getText().toString();
		int windowsToRead = (editText != null && !editText.isEmpty())
				? Integer.parseInt(editText) : WINDOWS_TO_RECORD;

		showToast("Starting to record...");

		while (currentIteration < windowsToRead * WINDOW_SIZE_IN_FRAMES) //960 = 15*64 -> 15*2s=30s
		{

			// read() kann entweder mind. buffer_size/2 zeichen zurŸckliefern
			// (wir brauchen viel weniger) oder blockiert:
			// http://stackoverflow.com/questions/15804903/android-dev-audiorecord-without-blocking-or-threads
			synchronized (this) {
				if (isRecording)
					readAudioSamples = audioRecorder.read(dataFrame16bit, 0, FRAME_SIZE_IN_SAMPLES);
				else
				{
					//we only get here in case the user kills us off early
					//remove last window as it's incomplete
					if (cepstrumWindow.size() > 0 && cepstrumWindow.size() < WINDOW_SIZE_IN_FRAMES)
						featureCepstrums.removeLast();
					//audioFeatures2csv(featureCepstrums, "features.csv");
					return;
				}
			}

			if (readAudioSamples == 0)
				return;
			
			if (!isFrameAdmittedByRMS(dataFrame16bit))
				continue;

			FrequencyProperties freq = processAudioFrame(readAudioSamples, dataFrame16bit, true);
			
			if (freq == null)
				continue;
			
			//combine WINDOW_SIZE_IN_FRAMES frames to a window
			//2s with a window size of 64
			if (cepstrumWindow.size() == WINDOW_SIZE_IN_FRAMES){
				cepstrumWindow = new ArrayList<double[]>(WINDOW_SIZE_IN_FRAMES);
				featureCepstrums.add(cepstrumWindow);

				psdWindow = new ArrayList<double[]>(WINDOW_SIZE_IN_FRAMES);
				psdsAcrossFrequencyBands.add(psdWindow);
			}
			
			currentIteration++;
			
			// Add PSDs of this frame to our window
			psdWindow.add(freq.getPsdAcrossFrequencyBands());
			// Add MFCCs of this frame to our window
			cepstrumWindow.add(freq.getFeatureCepstrum());
			
			if (cepstrumWindow.size() == WINDOW_SIZE_IN_FRAMES) //classify last window
				classifyWindow(cepstrumWindow);
		}

		showToast("Done recording.");
		/*for (ArrayList<double[]> windows : featureCepstrums) {

		}*/

		//audioFeatures2csv(featureCepstrums, "features.csv");
		return;
	}


	private void classifyWindow(ArrayList<double[]> cepstrumWindow) {
		PointList pl = new PointList(MFCCS_VALUE - 1);

		for (double[] frame : cepstrumWindow) {
			pl.add(frame);
		}

		double pFlo = gmmFlo.getLogLikelihood(pl);
		double pOther = gmmOther.getLogLikelihood(pl);

		//TODO
		if (pFlo > pOther)
			setRecognitionView(R.drawable.green_light);
		else
			setRecognitionView(R.drawable.red_light);
	}


	private void setRecognitionView(final int image) {
		runOnUiThread(new Runnable() 
		{                
			@Override
			public void run() 
			{
				recognition.setImageResource(image);
			}
		});
	}

	private void setAdmissionView(final int image) {
		runOnUiThread(new Runnable() 
		{                
			@Override
			public void run() 
			{
				recognition.setImageResource(image);
			}
		});
	}


	private void showToast(final String text) {
		runOnUiThread(new Runnable() 
		{                
			@Override
			public void run() 
			{
				Toast.makeText(getApplicationContext(), text, Toast.LENGTH_SHORT).show();
			}
		});
	}

	
	/**
	 * Takes an audio frame and processes it in the frequency domain:
	 * 1. Applies a Hamming smoothing window
	 * 2. Computes the FFT
	 * 3. Computes the Power Spectral Density for each frequency band
	 * 4. Computes the MFC coefficients
	 * 
	 * @param samples Number of samples in this frame (should be static)
	 * @param dataFrame16bit Array of samples
	 * @param dropIfBad Whether to drop the frame if the spectral entropy is below a threshold
	 * @return FrequencyProperties object containing FFT & MFCC coefs, PSDs;  null if dropped
	 * 
	 * based on code of Funf's AudioFeaturesProbe class
	 * 
	 * Funf: Open Sensing Framework
	 * Copyright (C) 2010-2011 Nadav Aharony, Wei Pan, Alex Pentland.
	 * Acknowledgments: Alan Gardner
	 * Contact: nadav@media.mit.edu
	 * 
	 * v1.0
	 */
	public FrequencyProperties processAudioFrame(int samples, short dataFrame16bit[], boolean dropIfBad) {
		double fftBufferR[] = new double[FFT_SIZE];
		double fftBufferI[] = new double[FFT_SIZE];
		
		double[] psdAcrossFrequencyBands = new double[FREQ_BANDEDGES.length - 1];
		double[] featureCepstrum = new double[MFCCS_VALUE-1];

		// Frequency analysis
		Arrays.fill(fftBufferR, 0);
		Arrays.fill(fftBufferI, 0);

		// Convert audio buffer to doubles
		for (int i = 0; i < samples; i++)
		{
			fftBufferR[i] = dataFrame16bit[i];
		}

		// In-place windowing
		featureWin.applyWindow(fftBufferR);

		// In-place FFT
		featureFFT.fft(fftBufferR, fftBufferI);
		
		if (dropIfBad && !isFrameAdmittedBySpectralEntropy(fftBufferR, fftBufferI))
			return null;

		// Get PSD across frequency band ranges
		for (int i = 0; i < (FREQ_BANDEDGES.length - 1); i ++)
		{
			int j = freqBandIdx[i];
			int k = freqBandIdx[i+1];
			double accum = 0;

			for (int h = j; h < k; h ++)
				accum += fftBufferR[h]*fftBufferR[h] + fftBufferI[h]*fftBufferI[h];

			psdAcrossFrequencyBands[i] = accum/((double)(k - j));
		}
		
		FrequencyProperties freq = new FrequencyProperties();
		
		freq.setFftImag(fftBufferI);
		freq.setFftReal(fftBufferR);
		freq.setFeatureCepstrum(featureCepstrum);
		freq.setPsdAcrossFrequencyBands(psdAcrossFrequencyBands);

		// Get MFCCs
		double[] featureCepstrumTemp = featureMFCC.cepstrum(fftBufferR, fftBufferI);

		// copy MFCCs
		for(int i = 1; i < featureCepstrumTemp.length; i++) {
			//only keep energy-independent features, drop first coefficient
			featureCepstrum[i-1] = featureCepstrumTemp[i];
		}
		
		return freq;
	}


	public double getRMS(short[] buffer) {
		double rms = 0;
		for (int i = 0; i < buffer.length; i++) {
			rms += buffer[i] * buffer[i];
		}
		return Math.sqrt(rms / buffer.length);
	}


	//TODO entropy for individual subbands
	//http://www.ee.uwa.edu.au/~roberto/research/theses/tr05-01.pdf
	//TODO double overflow? BigDecimal ok?

	/**
	 * Spectral entropy is calculated as
	 *
	 * 1) Normalize the power spectrum:
	 * Q(f)=P(f)/sum(P(f))  (where P(f) is the power spectrum)
	 * 
	 * 2) Transform with the Shannon function:
	 * H(f)=Q(f)[log(1/Q(f))]
	 * 
	 * 3) Spectral entropy:
	 * E=sum(H(f))/log(Nf)  (where Nf is the number of frequency components.
	 * 
	 * http://www.scielo.br/scielo.php?pid=S0034-70942004000300013&script=sci_arttext&tlng=en
	 * 
	 * @param fftReal
	 * @param fftImag
	 * @return
	 * 
	 * v1.1 Fixed "Non-terminating decimal expansion" error in divide()
	 *      by providing desired precision and rounding mode
	 */
	public static double getSpectralEntropy(double[] fftReal, double[] fftImag) {
		BigDecimal intensities[] = new BigDecimal[fftReal.length];
		BigDecimal sumOfIntensities = new BigDecimal(0d);
		double entropy = 0;
		
		try {
			//iterate over frequencies, compute intensities and overall sum of intensities
			for (int f = 0; f < fftReal.length; f++) {
				intensities[f] = new BigDecimal(fftReal[f]*fftReal[f] + fftImag[f]*fftImag[f]);
				sumOfIntensities = sumOfIntensities.add(intensities[f]);
			}
	
			for (BigDecimal intensity : intensities) {
				//normalize so that sum(p)=1 in order to get PMF
				BigDecimal p = intensity.divide(sumOfIntensities, 10, RoundingMode.HALF_UP);
				//only consider frequencies with p(f)>0
				if (p.compareTo(BigDecimal.ZERO) != 0) {
					entropy += p.doubleValue() * Math.log(p.doubleValue()) / Math.log(2);
				}
			}
			entropy *= -1;
			//optional: normalize
			entropy /= fftReal.length;
		} catch (Exception e) {
			e.printStackTrace();
		}
		return entropy;
	}


	/**
	 * 
	 * @param frame Array of features
	 * @return returns whether the frame is to be admitted
	 */
	public boolean isFrameAdmittedByRMS(short[] buffer) {
		//if (getRMS(buffer) < 0.5)
		//	return false;
		return true;
	}

	public boolean isFrameAdmittedBySpectralEntropy(double[] fftReal, double[] fftImag) {
		//if (getSpectralEntropy(fftReal,fftImag) > 0.5)
		//	return false;
		return true;
	}
	
	
	/**
	 * Reads from {@link #audioRecorder} and saves the stream to the sdcard.
	 * 
	 * based on code snippets taken from from
	 * http://eurodev.blogspot.de/2009/09/raw-audio-manipulation-in-android.html
	 */
	private void storeAudioStream() {
		File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + "/audioStream.pcm");
		int suffix = 0;

		while (file.exists())
			file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + "/audioStream_copy" + (suffix++) + ".pcm");
			//file.delete();

		// Create the new file.
		try {
			file.createNewFile();
		} catch (IOException e) {
			throw new IllegalStateException("Failed to create " + file.toString());
		}

		try {
			OutputStream os = new FileOutputStream(file);
			BufferedOutputStream bos = new BufferedOutputStream(os);
			DataOutputStream dos = new DataOutputStream(bos);

			short dataFrame16bit[] = new short[FRAME_SIZE_IN_SAMPLES];
			int readAudioSamples = 0;
			int currentIteration = 0;

			String editText = ((EditText) findViewById(R.id.editTextSamples)).getText().toString();
			int windowsToRead = (editText != null && !editText.isEmpty()) ?
					Integer.parseInt(editText) : WINDOWS_TO_RECORD;

			showToast("Starting to record...");

			while (currentIteration < windowsToRead * WINDOW_SIZE_IN_FRAMES) //960 = 15*64 -> 15*2s=30s
			{
				synchronized (this) {
					if (isRecording)
						readAudioSamples = audioRecorder.read(dataFrame16bit, 0, FRAME_SIZE_IN_SAMPLES);
					else {
						//we only get here in case the user kills us off early
						dos.close();
						return;
					}
				}

				currentIteration++;

				if (readAudioSamples == 0)
					return;

				if (currentIteration > WINDOW_SIZE_IN_FRAMES * DROP_FIRST_X_WINDOWS)
					for (int i = 0; i < readAudioSamples; i++)
						dos.writeShort(dataFrame16bit[i]);
				
			}
			showToast("Done recording.");
			dos.close();
		} catch (Throwable t) {
			Log.e(APP_NAME,"Recording Failed");
		}
	}


	/**
	 * Takes a list of windows of frames and stores them in a csv file on the sdcard.
	 * Each frame is represented by its MFCCs (without energy). Legacy-ish because
	 * we now store raw audio on the sdcard and do the processing elsewhere.
	 * 
	 * @param featuresByFrameByWindow Lists of MFCCs
	 * @param filename Name of the csv file
	 */
	public void audioFeatures2csv(LinkedList<ArrayList<double[]>> featuresByFrameByWindow, String filename) {
		PointList pl = new PointList(NUMBER_OF_FINAL_FEATURES);

		PrintWriter csvWriter;
		try
		{
			File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + "/" + filename);
			if(!file.exists())
				file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + "/" + filename);
			csvWriter = new  PrintWriter(new FileWriter(file,true));

			for (ArrayList<double[]> window : featuresByFrameByWindow) {
				for (double[] featuresByFrame : window) {
					pl.add(featuresByFrame);
					for (double d : featuresByFrame) {
						csvWriter.print(d + ",");
					}
					csvWriter.print("\r\n");
				}
			}

			csvWriter.close();

		}
		catch (Exception e)
		{
			e.printStackTrace();
		}
	}

}
